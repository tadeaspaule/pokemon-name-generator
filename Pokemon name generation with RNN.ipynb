{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Pokemon names with an RNN\n",
    "Using character-level prediction of what the next character should be given the last X characters, this model eventually tries to generate realistic-sounding Pokemon names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(819, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Input, concatenate, Reshape, Dropout\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tp_datasets import pokemon\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Pokemon names\n",
    "<i>Get tp_datasets (and therefore the Pokemon dataset) <a href=\"https://github.com/tadeaspaule/tp-datasets\">here</a></i>\n",
    "- First we get a list of names of all real Pokemon<br>\n",
    "- To remove noise in the model, I turned the names to lowercase and removed ~5ish Pokemon that have odd symbols in their name\n",
    "- We also want to know the length of the longest and shortest name, this will come up later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest name is 11 characters long\n",
      "Shortest name is 3 characters long\n"
     ]
    }
   ],
   "source": [
    "_,names,_ = pokemon.load_data(return_full_names=False)\n",
    "\n",
    "\n",
    "names = [name.lower() for name in names]\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "\n",
    "\n",
    "unwanted = [' ', \"'\", '-', '.', '2']\n",
    "def has_unwanted(word):\n",
    "    for char in word:\n",
    "        if char in unwanted:\n",
    "            return True\n",
    "    return False\n",
    "names = [name for name in names if not has_unwanted(name)]\n",
    "chars = [char for char in chars if char not in unwanted]\n",
    "\n",
    "char_index = dict([(chars[i],i) for i in range(len(chars))])\n",
    "maxlen = max([len(name) for name in names])\n",
    "minlen = min([len(name) for name in names])\n",
    "print(\"Longest name is\",maxlen,\"characters long\")\n",
    "print(\"Shortest name is\",minlen,\"characters long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the X - long sequences\n",
    "- This model basically works by looking at X characters (in this case 4), and predicting what the next character will be\n",
    "- Changing this X value will affect what patterns the model learns, if we make X too big it can simply memorize valid Pokemon names, but if we make it too small, it won't be able to accurately predict the next character\n",
    "- I played around with it a bit and settled on 4, but feel free to try out different values (you should only have to change the value below in seqlen = 4 and the rest of the code will adjust itself based on that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3569 sequences of length 4 made\n"
     ]
    }
   ],
   "source": [
    "# Making the input sequences\n",
    "\n",
    "seqlen = 4\n",
    "endchar = '/'\n",
    "assert(endchar not in chars)\n",
    "chars += endchar\n",
    "char_index[endchar] = len(chars) - 1\n",
    "sequences = []\n",
    "lengths = []     # To have the model learn a more macro understanding, it also takes the word's length so far\n",
    "                 # as input\n",
    "nextchars = []\n",
    "\n",
    "\n",
    "for name in names:\n",
    "    if len(name) < seqlen:\n",
    "        sequences.append(name + endchar*(seqlen - len(name)))\n",
    "        nextchars.append(endchar)\n",
    "        lengths.append(len(name))\n",
    "    else:\n",
    "        for i in range(0,len(name)-seqlen+1):\n",
    "            sequences.append(name[i:i+seqlen])\n",
    "            if i+seqlen < len(name):\n",
    "                nextchars.append(name[i+seqlen])\n",
    "            else:\n",
    "                nextchars.append(endchar)\n",
    "            lengths.append(i+seqlen)\n",
    "                \n",
    "\n",
    "print(len(sequences),\"sequences of length\",seqlen,\"made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding the sequences, word lengths, and next characters\n",
    "- One hot encoding means that, for example, if you have 5 characters that can appear, you turn the first character into [1 0 0 0 0], the second into [0 1 0 0 0], and so on\n",
    "- Here we do it with the sequences and with word lengths, because this format is easy for the model to read (and we need to somehow turn the sequence strings into number values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding the sequences\n",
    "# Also adding another input: the length of the word at the end of the sequence\n",
    "\n",
    "x = np.zeros(shape=(len(sequences),seqlen,len(chars)), dtype='float32')\n",
    "x2 = np.zeros(shape=(len(lengths),maxlen))\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    for j, char in enumerate(seq):\n",
    "        x[i,j,char_index[char]] = 1.\n",
    "\n",
    "for i, l in enumerate(lengths):\n",
    "    x2[i,l-1] = 1.\n",
    "\n",
    "y = np.zeros(shape=(len(nextchars),len(chars)))\n",
    "for i, char in enumerate(nextchars):\n",
    "    y[i,char_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for generating random starting sequences\n",
    "- Looks at the probabilities letters appear after each other (for example, how often is 'a' third when 'f' is second, compared to other letters that occur after a second 'f')\n",
    "- We will use this later to make brand new Pokemon names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictchars = [{} for _ in range(seqlen)]\n",
    "total = 0\n",
    "\n",
    "for name in names:\n",
    "    if len(name) < seqlen:\n",
    "        continue\n",
    "    total += 1\n",
    "    dictchars[0][name[0]] = dictchars[0].get(name[0],0) + 1\n",
    "    for i in range(1,seqlen):\n",
    "        if dictchars[i].get(name[i-1],0) == 0:\n",
    "            dictchars[i][name[i-1]] = {name[i]: 1}\n",
    "        elif dictchars[i][name[i-1]].get(name[i],0) == 0:\n",
    "            dictchars[i][name[i-1]][name[i]] = 1\n",
    "        else:\n",
    "            dictchars[i][name[i-1]][name[i]] += 1\n",
    "    \n",
    "'''\n",
    "What is dictchars?\n",
    "Basically, stores how often a letter occurs after another letter at a specific spot in a Pokemon name\n",
    "dictchars[0] just stores how often each letter is first, {a: 3, b:4, etc}\n",
    "\n",
    "dictchars[1+] store which letters (and how often) come after a certain letter.\n",
    "For example, if dictchars[1]['a'] = {b:4,c:1}, that means that if 'a' was first, \n",
    "b followed 4 times, while c followed only once.\n",
    "\n",
    "This is used in the method below to generate plausible-sounding starting sequences.\n",
    "'''\n",
    "    \n",
    "\n",
    "def generate_start_seq():\n",
    "    res = \"\" # The starting sequence will be stored here\n",
    "    p = sum([n for n in dictchars[0].values()]) # total amount of letter occurences\n",
    "    r = np.random.randint(0,p) # random number used to pick the next character\n",
    "    tot = 0\n",
    "    for key, item in dictchars[0].items():\n",
    "        if r >= tot and r < tot + item:\n",
    "            res += key\n",
    "            break\n",
    "        else:\n",
    "            tot += item\n",
    "\n",
    "    for i in range(1,seqlen):\n",
    "        ch = res[-1]\n",
    "        if dictchars[i].get(ch,0) == 0:\n",
    "            l = list(dictchars[i].keys())\n",
    "            ch = l[np.random.randint(0,len(l))]\n",
    "        p = sum([n for n in dictchars[i][ch].values()])\n",
    "        r = np.random.randint(0,p)\n",
    "        tot = 0\n",
    "        for key, item in dictchars[i][ch].items():\n",
    "            if r >= tot and r < tot + item:\n",
    "                res += key\n",
    "                break\n",
    "            else:\n",
    "                tot += item\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for generating text\n",
    "- The methods below basically take care of 'I give X letters, I get the full name', so that we can easily monitor the progress of the model (and combined with the above-declared method that makes random starting sequences, we won't even need to provide anything to get random Pokemon names)\n",
    "- There is one concept used below called <i>temperature</i>. Basically it's a measure randomness plays when selecting the next letter, with 0 being no randomness, always picking the most likely letter, and 1 being total randomness, and the letters are chosen based on their probability value\n",
    "- Adjusting this changes how your generated names look, typically the closer to 0 you are the more coherent and closely resembling the training data the output is, and the closer to 1 you are the more novel but sometimes also less coherent the output is. This mainly affects large generated texts though, not so much names. Nevertheless, I tend to go for ~0.4 temperature usually, but feel free to try out different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds,wordlength,temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    if temperature == 0:\n",
    "        # Avoiding a division by 0 error\n",
    "        return np.argmax(preds)\n",
    "    preds = np.log(preds) / temperature\n",
    "    \n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1,preds,1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_name(model,start,temperature=1.0,maxlength=maxlen):\n",
    "    res = start\n",
    "    inp = np.zeros(shape=(1,seqlen,len(chars)))\n",
    "    for i, char in enumerate(start):\n",
    "        inp[0,i,char_index[char]] = 1.\n",
    "    l = np.zeros(shape=(1,maxlen))\n",
    "    l[0,len(res)] = 1.\n",
    "    i = sample(model.predict(x=[inp,l])[0],len(res),temperature)\n",
    "    while i < len(chars)-1 and len(res) < maxlength:\n",
    "        res += chars[i]\n",
    "        start = start[1:] + chars[i]\n",
    "        inp = np.zeros(shape=(1,seqlen,len(chars)))\n",
    "        for i, char in enumerate(start):\n",
    "            inp[0,i,char_index[char]] = 1.\n",
    "        l = np.zeros(shape=(1,maxlen))\n",
    "        l[0,len(res)-1] = 1.\n",
    "        i = sample(model.predict(x=[inp,l])[0],len(res),temperature)\n",
    "    return res.title()\n",
    "\n",
    "def generate_random_name(model,temperature=0.3):\n",
    "    start = \"\"\n",
    "    while len(start) != seqlen:\n",
    "        try:\n",
    "            start = generate_start_seq()\n",
    "        except: pass\n",
    "    return generate_name(model,start,temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "- Here is where you can experiment and try out different approaches\n",
    "- After some testing, I went with the below setup:\n",
    "    - 2 Inputs (the sequence, and the one-hot-encoded length of the name at the end of that sequence)\n",
    "    - 2 parallel LSTM layers, one normal with relu, the other backwards with tanh, both dropout 0.4\n",
    "    - Concatenate the LSTM output with the one-hot-encoded length\n",
    "    - Dense output layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1 = Input(shape=x.shape[1:]) # sequence input\n",
    "inp2 = Input(shape=x2.shape[1:]) # length input\n",
    "lstm = LSTM(len(chars),activation='relu',dropout=0.3)(inp1)\n",
    "lstm2 = LSTM(len(chars),dropout=0.3,go_backwards=True)(inp1)\n",
    "concat = concatenate([lstm,lstm2,inp2])\n",
    "dense = Dense(len(chars),activation='softmax')(concat)\n",
    "\n",
    "model = Model([inp1,inp2],dense)\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for training a model and monitoring its progress\n",
    "- Using this makes it easy to try out different model architectures and see what names they are able to generate\n",
    "- For fast prototyping, just build and compile a model, then simply:\n",
    "```python\n",
    "try_model(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model(model,*,total_epochs=200,print_every=40,temperature=0.3,verbose=True):\n",
    "    for i in range(total_epochs//print_every):\n",
    "        history = model.fit([x,x2],y,\n",
    "                            epochs=print_every,\n",
    "                            batch_size=64,\n",
    "                            validation_split=0.05,\n",
    "                            verbose=0)\n",
    "        if verbose:\n",
    "            print(\"Epoch\",(i+1)*print_every)\n",
    "            print(\"First loss:            %1.4f\" % (history.history['loss'][0]))\n",
    "            print(\"Last loss:             %1.4f\" % (history.history['loss'][-1]))\n",
    "            print(\"First validation loss: %1.4f\" % (history.history['val_loss'][0]))\n",
    "            print(\"Last validation loss:  %1.4f\" % (history.history['val_loss'][-1]))\n",
    "            print()\n",
    "            print(\"Generating random names:\")\n",
    "            for _ in range(10):\n",
    "                print(generate_random_name(model,temperature))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, training the model and seeing how it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40\n",
      "First loss:            0.1553\n",
      "Last loss:             0.1125\n",
      "First validation loss: 0.1505\n",
      "Last validation loss:  0.1055\n",
      "\n",
      "Generating random names:\n",
      "Hoceon\n",
      "Gimeron\n",
      "Aruser\n",
      "Deas\n",
      "Manker\n",
      "Exer\n",
      "Nignle\n",
      "Aerfan\n",
      "Donchas\n",
      "Haxita\n",
      "\n",
      "Epoch 80\n",
      "First loss:            0.1125\n",
      "Last loss:             0.1062\n",
      "First validation loss: 0.1062\n",
      "Last validation loss:  0.0972\n",
      "\n",
      "Generating random names:\n",
      "Caphor\n",
      "Lulle\n",
      "Rumine\n",
      "Caile\n",
      "Misoron\n",
      "Goteet\n",
      "Spertich\n",
      "Felett\n",
      "Loltoe\n",
      "Chawitl\n",
      "\n",
      "Epoch 120\n",
      "First loss:            0.1060\n",
      "Last loss:             0.1015\n",
      "First validation loss: 0.0969\n",
      "Last validation loss:  0.0913\n",
      "\n",
      "Generating random names:\n",
      "Gayton\n",
      "Abratto\n",
      "Phiserit\n",
      "Notaro\n",
      "Colling\n",
      "Palasto\n",
      "Cocnoon\n",
      "Barp\n",
      "Chuckow\n",
      "Dimino\n",
      "\n",
      "Epoch 160\n",
      "First loss:            0.1013\n",
      "Last loss:             0.0975\n",
      "First validation loss: 0.0907\n",
      "Last validation loss:  0.0853\n",
      "\n",
      "Generating random names:\n",
      "Clepin\n",
      "Gountar\n",
      "Kyurea\n",
      "Shrriza\n",
      "Vanaur\n",
      "Seedoin\n",
      "Luboar\n",
      "Tofatho\n",
      "Beim\n",
      "Mamitar\n",
      "\n",
      "Epoch 200\n",
      "First loss:            0.0981\n",
      "Last loss:             0.0942\n",
      "First validation loss: 0.0850\n",
      "Last validation loss:  0.0777\n",
      "\n",
      "Generating random names:\n",
      "Tewking\n",
      "Ledino\n",
      "Klalion\n",
      "Rerdoros\n",
      "Tyree\n",
      "Voturn\n",
      "Amblaria\n",
      "Pelbut\n",
      "Micarion\n",
      "Togmar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To generate more afterwards:\n",
    "```python\n",
    "for _ in range(100):\n",
    "    print(generate_random_name(model,temperature=0.4))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To save the model and use it later:\n",
    "```python\n",
    "model.save('pokename.h5') # Save the model you like...\n",
    "```\n",
    "And at a later time just get it back like this:<br>\n",
    "(This assumes the .h5 file is in the current directory, otherwise specify the path to it)\n",
    "```python\n",
    "m = load_model('pokename.h5')\n",
    "for _ in range(10):\n",
    "    print(generate_random_name(m,temperature=0.4))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To use your own starting sequence:\n",
    "```python\n",
    "my_starting_sequence = \"Tyrm\"\n",
    "pokename = generate_name(model,my_starting_sequence)\n",
    "print(pokename)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
